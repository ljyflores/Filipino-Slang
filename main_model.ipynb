{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"SMR6g9M8ZyS0"},"outputs":[],"source":["!pip install transformers\n","!pip install textattack\n","!pip install tensorflow_text\n","!cp /content/drive/MyDrive/Slang_Project/utils.py ."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ru883869RDfF"},"outputs":[],"source":["import pandas as pd\n","import string\n","import re\n","\n","from torch import nn\n","import torch\n","\n","from transformers import T5ForConditionalGeneration, AutoTokenizer\n","from transformers import CanineTokenizer, CanineForQuestionAnswering\n","from transformers import BertTokenizer, BertLMHeadModel\n","from transformers import T5ForConditionalGeneration\n","from transformers import AutoModelForSeq2SeqLM\n","from transformers import AdamW\n","\n","from utils import *\n","\n","device = torch.device(\"cuda\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8SxOjeDOtCqT"},"outputs":[],"source":["def encode_utf8(s, num_special_tokens):\n","  return torch.tensor([list(s.encode(\"utf-8\"))]) + num_special_tokens\n","\n","def decode_utf8(s, num_special_tokens):\n","  s = (s-num_special_tokens).numpy()[0]\n","  return ''.join(map(chr, s))"]},{"cell_type":"code","execution_count":26,"metadata":{"executionInfo":{"elapsed":204,"status":"ok","timestamp":1656107526112,"user":{"displayName":"Lj Flores","userId":"11095487892395270199"},"user_tz":240},"id":"wUZc8PD2iAMs"},"outputs":[],"source":["SOURCE_PATH = \"/content/drive/MyDrive/Slang_Project/\"\n","USE_BERT = False"]},{"cell_type":"code","execution_count":112,"metadata":{"executionInfo":{"elapsed":182,"status":"ok","timestamp":1656125176378,"user":{"displayName":"Lj Flores","userId":"11095487892395270199"},"user_tz":240},"id":"rStFLkiS-YMb"},"outputs":[],"source":["df = pd.read_csv(SOURCE_PATH + \"data/train_words.csv\", header=None)\n","df[0] = df[0].apply(remove_name)\n","df = df.applymap(lambda x: x.lower().translate(str.maketrans(' ', ' ', string.punctuation+\"0123456789·\")))\n","\n","df_test    = pd.read_csv(SOURCE_PATH + \"data/test_words.csv\", header=None)\n","df_test[0] = df_test[0].apply(remove_name)\n","df_test    = df_test.applymap(lambda x: x.lower().translate(str.maketrans(' ', ' ', string.punctuation+\"0123456789·\")))\n","\n","dict_tl = pd.read_csv(SOURCE_PATH + \"data/tl-en.txt\", header=None, delimiter='\\t')\n","vocab_tl = set(dict_tl[0])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VI3WQEkxU2-o"},"outputs":[],"source":["# Initialize tokenizers, model, loss, optimizer\n","if USE_BERT:\n","  tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-uncased')\n","  model = BertLMHeadModel.from_pretrained(\"bert-base-multilingual-uncased\").to(device)\n","else:\n","  model = T5ForConditionalGeneration.from_pretrained(\"google/byt5-small\").to(device)\n","  tokenizer = AutoTokenizer.from_pretrained(\"google/byt5-small\",\n","                                            output_scores=True,\n","                                            output_hidden_states=True)\n","\n","nll_loss = nn.CrossEntropyLoss()\n","optimizer = AdamW(model.parameters(),\n","                  lr = 5e-5, # args.learning_rate - default is 5e-5\n","                  eps = 1e-8 # args.adam_epsilon  - default is 1e-8.\n","                )"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HtDk2OlaoXwo"},"outputs":[],"source":["model.train()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FJYR2z9TTTIN"},"outputs":[],"source":["for idx in range(1000):\n","  loss, steps = 0.0, 0\n","\n","  for i in range(df.shape[0]):\n","    model.zero_grad()\n","\n","    input = tokenizer(list(df.iloc[i]), return_tensors='pt', padding=True).to(device)\n","    output = model(input_ids    = input.input_ids[0].unsqueeze(0),\n","                  attention_mask = input.attention_mask[0].unsqueeze(0),\n","                  labels         = input.input_ids[1].unsqueeze(0))\n","\n","    output.loss.backward()\n","    optimizer.step()\n","    loss += float(output.loss)\n","    steps += 1\n","    # print(output.loss)\n","    # print(tokenizer.decode(torch.argmax(output.logits, axis=2)[0]))\n","    # print(df.iloc[i][1])\n","  print(idx, loss/steps)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":407},"executionInfo":{"elapsed":402,"status":"error","timestamp":1655434384729,"user":{"displayName":"Lj Flores","userId":"11095487892395270199"},"user_tz":240},"id":"n-jdDRWZlxM8","outputId":"c8e42dca-231e-4b66-f112-6dea9de2c46b"},"outputs":[{"ename":"ValueError","evalue":"ignored","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m<ipython-input-30-01a662308522>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m   output_1 = model(input_ids      = input_1.input_ids[0].unsqueeze(0),\n\u001b[0;32m---> 14\u001b[0;31m                    attention_mask = input_1.attention_mask[0].unsqueeze(0))\n\u001b[0m\u001b[1;32m     15\u001b[0m   output_2 = model.generate(input_ids      = input_2.input_ids[0].unsqueeze(0),\n\u001b[1;32m     16\u001b[0m                    attention_mask = input_2.attention_mask[0].unsqueeze(0))\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1108\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1111\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1112\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/models/t5/modeling_t5.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, decoder_input_ids, decoder_attention_mask, head_mask, decoder_head_mask, cross_attn_head_mask, encoder_outputs, past_key_values, inputs_embeds, decoder_inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1618\u001b[0m             \u001b[0moutput_attentions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_attentions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1619\u001b[0m             \u001b[0moutput_hidden_states\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_hidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1620\u001b[0;31m             \u001b[0mreturn_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreturn_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1621\u001b[0m         )\n\u001b[1;32m   1622\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1108\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1111\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1112\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/models/t5/modeling_t5.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, encoder_hidden_states, encoder_attention_mask, inputs_embeds, head_mask, cross_attn_head_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    898\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    899\u001b[0m             \u001b[0merr_msg_prefix\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"decoder_\"\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_decoder\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 900\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"You have to specify either {err_msg_prefix}input_ids or {err_msg_prefix}inputs_embeds\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    901\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    902\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0minputs_embeds\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mValueError\u001b[0m: You have to specify either decoder_input_ids or decoder_inputs_embeds"]}],"source":["for j in range(df_test.shape[0]):\n","\n","  i=10\n","  model.zero_grad()\n","  \n","  input_1 = tokenizer(df_test[0][i], \n","                      return_tensors='pt', \n","                      padding=True).to(device)\n","  input_2 = tokenizer(perturb_test_sent(df_test[0][i], vocab_tl), \n","                      return_tensors='pt', \n","                      padding=True).to(device)\n","\n","  output_1 = model(input_ids      = input_1.input_ids[0].unsqueeze(0),\n","                   attention_mask = input_1.attention_mask[0].unsqueeze(0))\n","  output_2 = model.generate(input_ids = input_2.input_ids[0].unsqueeze(0),\n","                   attention_mask = input_2.attention_mask[0].unsqueeze(0))\n","  \n","  # Backpropagate Squared Diff\n","  min_idx     = min(output_1.logits.shape[1],output_2.logits.shape[1])\n","  diff_tensor = output_1.logits[:,:min_idx,:]-output_2.logits[:,:min_idx,:]\n","  torch.mean(diff_tensor**2).backward()\n","\n","  print(\"Orig: \"+df_test[0][i])\n","  print(\"Output 1: \"+tokenizer.decode(torch.argmax(output_1.logits, axis=2)[0]))\n","  print(\"Output 1: \"+tokenizer.decode(torch.argmax(output_2.logits, axis=2)[0]))"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"executionInfo":{"elapsed":178,"status":"ok","timestamp":1655434386664,"user":{"displayName":"Lj Flores","userId":"11095487892395270199"},"user_tz":240},"id":"igJ_t_Xvj9gC","outputId":"90b075a6-4608-4786-f8bb-cb41b4841951"},"outputs":[{"data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'ung</s>'"]},"execution_count":31,"metadata":{},"output_type":"execute_result"}],"source":["tokenizer.decode(input_1.input_ids[0])"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"executionInfo":{"elapsed":266,"status":"ok","timestamp":1655434387069,"user":{"displayName":"Lj Flores","userId":"11095487892395270199"},"user_tz":240},"id":"7W1YQfQtYkZ7","outputId":"db73b5d2-f09a-41c0-80f5-718b0ec7e551"},"outputs":[{"data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'<pad>an</s>'"]},"execution_count":32,"metadata":{},"output_type":"execute_result"}],"source":["tokenizer.decode(model.generate(input_ids = input_1.input_ids[0].unsqueeze(0),\n","                                attention_mask = input_1.attention_mask[0].unsqueeze(0),\n","                                output_scores=True,\n","                                output_hidden_states=True)[0])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-NcaHsVUE73R"},"outputs":[],"source":["model.generate()"]}],"metadata":{"colab":{"authorship_tag":"ABX9TyNWQnRm+uqiuAInYaP0af/M","collapsed_sections":[],"mount_file_id":"1hWEbrphNNAgInXXHP2Lm4F1-gQlsT5nF","name":"main.ipynb","provenance":[]},"gpuClass":"standard","kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.2"}},"nbformat":4,"nbformat_minor":0}
