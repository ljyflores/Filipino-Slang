{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"id":"ru883869RDfF"},"outputs":[{"name":"stderr","output_type":"stream","text":["2022-07-06 23:36:30.384630: I tensorflow/core/util/util.cc:169] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n"]}],"source":["import pandas as pd\n","import numpy as np\n","import ast\n","import random\n","\n","from torch import nn\n","import torch\n","\n","from transformers import T5ForConditionalGeneration, AutoTokenizer\n","from transformers import BertTokenizer, BertLMHeadModel\n","from transformers import T5ForConditionalGeneration\n","from transformers import AdamW\n","\n","from utils import perturb_test_sent, evaluate as evaluate_results\n","\n","device = torch.device(\"cuda\")"]},{"cell_type":"code","execution_count":2,"metadata":{"executionInfo":{"elapsed":182,"status":"ok","timestamp":1656125176378,"user":{"displayName":"Lj Flores","userId":"11095487892395270199"},"user_tz":240},"id":"rStFLkiS-YMb"},"outputs":[],"source":["SOURCE_PATH = \"\"\n","\n","# Load train/test datasets\n","df      = pd.read_csv(SOURCE_PATH + \"data/train_words.csv\", header=None)\n","df_val  = pd.read_csv(SOURCE_PATH + \"data/test_words.csv\", header=None)\n","\n","# Load vocab\n","f = open(\"TagalogStemmerPython/output/with_info.txt\", \"r\", encoding='latin1')\n","f = f.readlines()\n","vocab_tl = set(ast.literal_eval(item.strip('\\n'))['word'] for item in f)\n","vocab_tl = set(df[1]).union(vocab_tl) # Add in vocab from dataframe\n","vocab_tl = set(df_val[1]).union(vocab_tl) # Add in vocab from test dataframe"]},{"cell_type":"code","execution_count":3,"metadata":{},"outputs":[],"source":["USE_BERT = False"]},{"cell_type":"code","execution_count":7,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["/tmp/ipykernel_2240412/1769001629.py:3: FutureWarning: Passing a set as an indexer is deprecated and will raise in a future version. Use a list instead.\n","  df_train = df.loc[set(range(df.shape[0])).difference(test_idx)].reset_index(drop=True)\n"]}],"source":["# Split into train and test\n","test_idx = random.sample(range(df.shape[0]), round(df.shape[0]/5))\n","df_train = df.loc[set(range(df.shape[0])).difference(test_idx)].reset_index(drop=True)\n","df_test  = df.loc[test_idx].reset_index(drop=True)"]},{"cell_type":"code","execution_count":55,"metadata":{},"outputs":[],"source":["def train(perturb=False, mse_weight=0.5):\n","    model.train()\n","    loss, steps = 0.0, 0.0\n","    \n","    for i in range(df_train.shape[0]):\n","        model.zero_grad()\n","\n","        input = tokenizer(list(df_train.iloc[i]), return_tensors='pt', padding=True).to(device)\n","        output = model(input_ids      = input.input_ids[0].unsqueeze(0),\n","                       attention_mask = input.attention_mask[0].unsqueeze(0),\n","                       labels         = input.input_ids[1].unsqueeze(0))\n","        \n","        if perturb:\n","            input2 = tokenizer([perturb_test_sent(df_train.iloc[i,0], vocab_tl), \n","                                df_train.iloc[i,1]], \n","                        return_tensors='pt', \n","                        padding=True).to(device)\n","            output2 = model(input_ids      = input2.input_ids[0].unsqueeze(0),\n","                            attention_mask = input2.attention_mask[0].unsqueeze(0),\n","                            labels         = input2.input_ids[1].unsqueeze(0))\n","        \n","        if perturb:\n","            # Compute squared diff loss\n","            min_idx     = min(output.logits.shape[1],output2.logits.shape[1])\n","            diff_tensor = output.logits[:,:min_idx,:]-output2.logits[:,:min_idx,:]\n","            mse_loss    = torch.sqrt(torch.mean(diff_tensor**2)/output.logits.shape[0])\n","            \n","            # Compute total loss\n","            total_loss = (mse_weight*mse_loss)+((1-mse_weight)*output.loss)\n","            \n","            total_loss.backward()\n","            optimizer.step()\n","            loss += float(total_loss)\n","            \n","        else:\n","            output.loss.backward()\n","            optimizer.step()\n","            loss += float(output.loss)\n","        steps += 1\n","    \n","    return loss/steps\n","\n","def evaluate():\n","    loss, steps = 0.0, 0.0\n","    with torch.no_grad():\n","        for i in range(df_test.shape[0]):\n","            input = tokenizer(list(df_test.iloc[i]), return_tensors='pt', padding=True).to(device)\n","            output = model(input_ids      = input.input_ids[0].unsqueeze(0),\n","                           attention_mask = input.attention_mask[0].unsqueeze(0),\n","                           labels         = input.input_ids[1].unsqueeze(0))\n","            loss += output.loss\n","            steps += 1\n","    return loss/steps\n","\n","def clean_word(s):\n","    return s.replace('<pad>','').replace('</s>','')\n","\n","# Generate top 5 words per candidate\n","def generate_k_candidates(dataframe, k=5):\n","    result = []\n","    with torch.no_grad():\n","        for i in range(dataframe.shape[0]):\n","            input = tokenizer(list(dataframe.iloc[i]), return_tensors='pt', padding=True).to(device)\n","            output = model.generate(input_ids = input.input_ids[0].unsqueeze(0),\n","                                    attention_mask = input.attention_mask[0].unsqueeze(0),\n","                                    num_return_sequences = k,\n","                                    num_beams = k)\n","            output = tokenizer.batch_decode(output)\n","            output = list(map(clean_word, output))\n","            result.append(output)\n","    return result\n","\n","def initialize(USE_BERT, lr=5e-5, eps=1e-8):\n","    # Initialize tokenizers, model, loss, optimizer\n","    if USE_BERT:\n","        tokenizer.from_pretrained('bert-base-multilingual-uncased')\n","        model = BertLMHeadModel.from_pretrained(\"bert-base-multilingual-uncased\").to(device)\n","    else:\n","        model = T5ForConditionalGeneration.from_pretrained(\"google/byt5-small\").to(device)\n","        tokenizer = AutoTokenizer.from_pretrained(\"google/byt5-small\",\n","                                                  output_scores=True,\n","                                                  output_hidden_states=True)\n","\n","    nll_loss = nn.CrossEntropyLoss()\n","    optimizer = AdamW(model.parameters(),\n","                      lr = lr, # args.learning_rate - default is 5e-5\n","                      eps = eps # args.adam_epsilon  - default is 1e-8.\n","                    )\n","    return model, tokenizer, nll_loss, optimizer"]},{"cell_type":"markdown","metadata":{},"source":["Normal Training Setting"]},{"cell_type":"code","execution_count":9,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch 1; Train: 6.208001191951027; Test: 3.793813467025757\n","Epoch 2; Train: 2.9353004282663675; Test: 2.5620150566101074\n","Epoch 3; Train: 2.3688321778596926; Test: 2.276390314102173\n","Epoch 4; Train: 1.9996028182920345; Test: 1.9397112131118774\n","Epoch 5; Train: 1.7801968050150832; Test: 1.8884611129760742\n","Epoch 6; Train: 1.5986439016732303; Test: 1.5805171728134155\n","Epoch 7; Train: 1.3676149255234349; Test: 1.4615607261657715\n","Epoch 8; Train: 1.1776631346418838; Test: 1.3617585897445679\n","Epoch 9; Train: 1.0113097660643748; Test: 1.2640570402145386\n","Epoch 10; Train: 0.8564147209394569; Test: 1.1551393270492554\n","Epoch 11; Train: 0.7850987756363124; Test: 1.0066871643066406\n","Epoch 12; Train: 0.7518448811024427; Test: 0.9231386184692383\n","Epoch 13; Train: 0.6079841941087083; Test: 0.8400849103927612\n","0.5187350302734528 tensor(0.8553, device='cuda:0') 13\n"]}],"source":["model, tokenizer, nll_loss, optimizer = initialize(USE_BERT=False)\n","\n","best_val_loss = np.inf\n","epochs = 0\n","\n","while True:\n","    train_loss = train()\n","    val_loss   = evaluate()\n","    if val_loss < best_val_loss:\n","        best_val_loss = val_loss\n","        epochs += 1\n","        print(f\"Epoch {epochs}; Train: {train_loss}; Test: {val_loss}\")\n","    else:\n","        print(train_loss, val_loss, epochs)\n","        break"]},{"cell_type":"code","execution_count":23,"metadata":{},"outputs":[{"data":{"text/plain":["{'best_dl': 1.19,\n"," 'max_dl': 4.44,\n"," 'avg_dl': 2.8239999999999994,\n"," 'acc_1': 0.27,\n"," 'acc_3': 0.45,\n"," 'acc_5': 0.52,\n"," 'target_in_candidate': 0.52}"]},"execution_count":23,"metadata":{},"output_type":"execute_result"}],"source":["# Results for normal training setting\n","output_lst = generate_k_candidates(df_val, 5)\n","evaluate_results(output_lst, df_val[1])"]},{"cell_type":"markdown","metadata":{},"source":["Semi-Supervised Setting"]},{"cell_type":"code","execution_count":59,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch 1; Train: 11.056835768636594; Test: 4.690049648284912\n","Epoch 2; Train: 7.1538529445317165; Test: 3.3120205402374268\n","Epoch 3; Train: 4.94390930420111; Test: 2.469358444213867\n","Epoch 4; Train: 3.9842169107484424; Test: 2.3854293823242188\n","Epoch 5; Train: 2.888489823696042; Test: 2.349212646484375\n","Epoch 6; Train: 2.4572199257937344; Test: 2.2144558429718018\n","Epoch 7; Train: 2.136661278314827; Test: 2.103837490081787\n","Epoch 8; Train: 1.9581665795696668; Test: 2.041865825653076\n","Epoch 9; Train: 1.793297808771291; Test: 1.8708345890045166\n","Epoch 10; Train: 1.7174554580499317; Test: 1.824424386024475\n","Epoch 11; Train: 1.5974195185771658; Test: 1.7025080919265747\n","Epoch 12; Train: 1.508887403021174; Test: 1.5896350145339966\n","Epoch 13; Train: 1.4078386083614727; Test: 1.4664043188095093\n","Epoch 14; Train: 1.3340068538326861; Test: 1.3377220630645752\n","Epoch 15; Train: 1.2421415347205707; Test: 1.225014090538025\n","Epoch 16; Train: 1.1677047228517612; Test: 1.1890171766281128\n","Epoch 17; Train: 1.1082794900275459; Test: 1.1717227697372437\n","Epoch 18; Train: 1.0480389016226304; Test: 1.1353503465652466\n","Epoch 19; Train: 1.0077486418741795; Test: 1.1190104484558105\n","Epoch 20; Train: 0.9466385033505023; Test: 1.034529685974121\n","Epoch 21; Train: 0.9345814676816798; Test: 0.9998272657394409\n","Epoch 22; Train: 0.8800645511258732; Test: 0.9414531588554382\n","0.8506160372298611 tensor(0.9643, device='cuda:0') 22\n"]}],"source":["model, tokenizer, nll_loss, optimizer = initialize(USE_BERT=False)\n","\n","best_val_loss = np.inf\n","epochs = 0\n","\n","while True:\n","    train_loss = train(True, 0.4)\n","    val_loss   = evaluate()\n","    if val_loss < best_val_loss:\n","        best_val_loss = val_loss\n","        epochs += 1\n","        print(f\"Epoch {epochs}; Train: {train_loss}; Test: {val_loss}\")\n","    else:\n","        print(train_loss, val_loss, epochs)\n","        break"]},{"cell_type":"code","execution_count":60,"metadata":{},"outputs":[{"data":{"text/plain":["{'best_dl': 0.94,\n"," 'max_dl': 4.27,\n"," 'avg_dl': 2.7039999999999993,\n"," 'acc_1': 0.31,\n"," 'acc_3': 0.46,\n"," 'acc_5': 0.54,\n"," 'target_in_candidate': 0.54}"]},"execution_count":60,"metadata":{},"output_type":"execute_result"}],"source":["# Results for semi-supervised training setting\n","output_lst = generate_k_candidates(df_val, 5)\n","evaluate_results(output_lst, df_val[1])"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"colab":{"authorship_tag":"ABX9TyNWQnRm+uqiuAInYaP0af/M","collapsed_sections":[],"mount_file_id":"1hWEbrphNNAgInXXHP2Lm4F1-gQlsT5nF","name":"main.ipynb","provenance":[]},"gpuClass":"standard","kernelspec":{"display_name":"Python 3.9.5 ('base': conda)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.5"},"vscode":{"interpreter":{"hash":"0d78cf5354c54536ee2fe2974b55665bb4fd5d446126f0c5d0792c4750b1da66"}}},"nbformat":4,"nbformat_minor":0}
