{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"id":"ru883869RDfF"},"outputs":[{"name":"stderr","output_type":"stream","text":["/Users/lorenzoflores/.pyenv/versions/3.8.0/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n","  from .autonotebook import tqdm as notebook_tqdm\n","/Users/lorenzoflores/.pyenv/versions/3.8.0/lib/python3.8/site-packages/fuzzywuzzy/fuzz.py:11: UserWarning: Using slow pure-python SequenceMatcher. Install python-Levenshtein to remove this warning\n","  warnings.warn('Using slow pure-python SequenceMatcher. Install python-Levenshtein to remove this warning')\n"]}],"source":["import pandas as pd\n","import ast\n","\n","from torch import nn\n","import torch\n","\n","from transformers import T5ForConditionalGeneration, AutoTokenizer\n","from transformers import BertTokenizer, BertLMHeadModel\n","from transformers import T5ForConditionalGeneration\n","from transformers import AdamW\n","\n","from utils import *\n","\n","device = torch.device(\"cuda\")"]},{"cell_type":"code","execution_count":2,"metadata":{"id":"8SxOjeDOtCqT"},"outputs":[],"source":["def encode_utf8(s, num_special_tokens):\n","  return torch.tensor([list(s.encode(\"utf-8\"))]) + num_special_tokens\n","\n","def decode_utf8(s, num_special_tokens):\n","  s = (s-num_special_tokens).numpy()[0]\n","  return ''.join(map(chr, s))"]},{"cell_type":"code","execution_count":3,"metadata":{"executionInfo":{"elapsed":182,"status":"ok","timestamp":1656125176378,"user":{"displayName":"Lj Flores","userId":"11095487892395270199"},"user_tz":240},"id":"rStFLkiS-YMb"},"outputs":[],"source":["SOURCE_PATH = \"\"\n","\n","# Load train/test datasets\n","df      = pd.read_csv(SOURCE_PATH + \"data/train_words.csv\", header=None)\n","df_test = pd.read_csv(SOURCE_PATH + \"data/test_words.csv\", header=None)\n","\n","# Load vocab\n","f = open(\"TagalogStemmerPython/output/with_info.txt\", \"r\", encoding='latin1')\n","f = f.readlines()\n","vocab_tl = set(ast.literal_eval(item.strip('\\n'))['word'] for item in f)\n","vocab_tl = set(df[1]).union(vocab_tl) # Add in vocab from dataframe\n","vocab_tl = set(df_test[1]).union(vocab_tl) # Add in vocab from test dataframe"]},{"cell_type":"code","execution_count":4,"metadata":{},"outputs":[],"source":["USE_BERT = False"]},{"cell_type":"code","execution_count":5,"metadata":{"id":"VI3WQEkxU2-o"},"outputs":[{"name":"stderr","output_type":"stream","text":["Downloading: 100%|██████████| 698/698 [00:00<00:00, 70.0kB/s]\n","Downloading: 100%|██████████| 1.12G/1.12G [00:47<00:00, 25.1MB/s]\n"]},{"ename":"AssertionError","evalue":"Torch not compiled with CUDA enabled","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)","\u001b[1;32m/Users/lorenzoflores/Documents/GitHub/Filipino-Slang/main_model.ipynb Cell 5'\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/lorenzoflores/Documents/GitHub/Filipino-Slang/main_model.ipynb#ch0000005?line=3'>4</a>\u001b[0m   model \u001b[39m=\u001b[39m BertLMHeadModel\u001b[39m.\u001b[39mfrom_pretrained(\u001b[39m\"\u001b[39m\u001b[39mbert-base-multilingual-uncased\u001b[39m\u001b[39m\"\u001b[39m)\u001b[39m.\u001b[39mto(device)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/lorenzoflores/Documents/GitHub/Filipino-Slang/main_model.ipynb#ch0000005?line=4'>5</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/lorenzoflores/Documents/GitHub/Filipino-Slang/main_model.ipynb#ch0000005?line=5'>6</a>\u001b[0m   model \u001b[39m=\u001b[39m T5ForConditionalGeneration\u001b[39m.\u001b[39;49mfrom_pretrained(\u001b[39m\"\u001b[39;49m\u001b[39mgoogle/byt5-small\u001b[39;49m\u001b[39m\"\u001b[39;49m)\u001b[39m.\u001b[39;49mto(device)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/lorenzoflores/Documents/GitHub/Filipino-Slang/main_model.ipynb#ch0000005?line=6'>7</a>\u001b[0m   tokenizer \u001b[39m=\u001b[39m AutoTokenizer\u001b[39m.\u001b[39mfrom_pretrained(\u001b[39m\"\u001b[39m\u001b[39mgoogle/byt5-small\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/lorenzoflores/Documents/GitHub/Filipino-Slang/main_model.ipynb#ch0000005?line=7'>8</a>\u001b[0m                                             output_scores\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m,\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/lorenzoflores/Documents/GitHub/Filipino-Slang/main_model.ipynb#ch0000005?line=8'>9</a>\u001b[0m                                             output_hidden_states\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/lorenzoflores/Documents/GitHub/Filipino-Slang/main_model.ipynb#ch0000005?line=10'>11</a>\u001b[0m nll_loss \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mCrossEntropyLoss()\n","File \u001b[0;32m~/.pyenv/versions/3.8.0/lib/python3.8/site-packages/torch/nn/modules/module.py:927\u001b[0m, in \u001b[0;36mModule.to\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    923\u001b[0m         \u001b[39mreturn\u001b[39;00m t\u001b[39m.\u001b[39mto(device, dtype \u001b[39mif\u001b[39;00m t\u001b[39m.\u001b[39mis_floating_point() \u001b[39mor\u001b[39;00m t\u001b[39m.\u001b[39mis_complex() \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m    924\u001b[0m                     non_blocking, memory_format\u001b[39m=\u001b[39mconvert_to_format)\n\u001b[1;32m    925\u001b[0m     \u001b[39mreturn\u001b[39;00m t\u001b[39m.\u001b[39mto(device, dtype \u001b[39mif\u001b[39;00m t\u001b[39m.\u001b[39mis_floating_point() \u001b[39mor\u001b[39;00m t\u001b[39m.\u001b[39mis_complex() \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m, non_blocking)\n\u001b[0;32m--> 927\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_apply(convert)\n","File \u001b[0;32m~/.pyenv/versions/3.8.0/lib/python3.8/site-packages/torch/nn/modules/module.py:579\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    577\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_apply\u001b[39m(\u001b[39mself\u001b[39m, fn):\n\u001b[1;32m    578\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mchildren():\n\u001b[0;32m--> 579\u001b[0m         module\u001b[39m.\u001b[39;49m_apply(fn)\n\u001b[1;32m    581\u001b[0m     \u001b[39mdef\u001b[39;00m \u001b[39mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    582\u001b[0m         \u001b[39mif\u001b[39;00m torch\u001b[39m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    583\u001b[0m             \u001b[39m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    584\u001b[0m             \u001b[39m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    589\u001b[0m             \u001b[39m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    590\u001b[0m             \u001b[39m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n","File \u001b[0;32m~/.pyenv/versions/3.8.0/lib/python3.8/site-packages/torch/nn/modules/module.py:602\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    598\u001b[0m \u001b[39m# Tensors stored in modules are graph leaves, and we don't want to\u001b[39;00m\n\u001b[1;32m    599\u001b[0m \u001b[39m# track autograd history of `param_applied`, so we have to use\u001b[39;00m\n\u001b[1;32m    600\u001b[0m \u001b[39m# `with torch.no_grad():`\u001b[39;00m\n\u001b[1;32m    601\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n\u001b[0;32m--> 602\u001b[0m     param_applied \u001b[39m=\u001b[39m fn(param)\n\u001b[1;32m    603\u001b[0m should_use_set_data \u001b[39m=\u001b[39m compute_should_use_set_data(param, param_applied)\n\u001b[1;32m    604\u001b[0m \u001b[39mif\u001b[39;00m should_use_set_data:\n","File \u001b[0;32m~/.pyenv/versions/3.8.0/lib/python3.8/site-packages/torch/nn/modules/module.py:925\u001b[0m, in \u001b[0;36mModule.to.<locals>.convert\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m    922\u001b[0m \u001b[39mif\u001b[39;00m convert_to_format \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m t\u001b[39m.\u001b[39mdim() \u001b[39min\u001b[39;00m (\u001b[39m4\u001b[39m, \u001b[39m5\u001b[39m):\n\u001b[1;32m    923\u001b[0m     \u001b[39mreturn\u001b[39;00m t\u001b[39m.\u001b[39mto(device, dtype \u001b[39mif\u001b[39;00m t\u001b[39m.\u001b[39mis_floating_point() \u001b[39mor\u001b[39;00m t\u001b[39m.\u001b[39mis_complex() \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m    924\u001b[0m                 non_blocking, memory_format\u001b[39m=\u001b[39mconvert_to_format)\n\u001b[0;32m--> 925\u001b[0m \u001b[39mreturn\u001b[39;00m t\u001b[39m.\u001b[39;49mto(device, dtype \u001b[39mif\u001b[39;49;00m t\u001b[39m.\u001b[39;49mis_floating_point() \u001b[39mor\u001b[39;49;00m t\u001b[39m.\u001b[39;49mis_complex() \u001b[39melse\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m, non_blocking)\n","File \u001b[0;32m~/.pyenv/versions/3.8.0/lib/python3.8/site-packages/torch/cuda/__init__.py:211\u001b[0m, in \u001b[0;36m_lazy_init\u001b[0;34m()\u001b[0m\n\u001b[1;32m    207\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\n\u001b[1;32m    208\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mCannot re-initialize CUDA in forked subprocess. To use CUDA with \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    209\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mmultiprocessing, you must use the \u001b[39m\u001b[39m'\u001b[39m\u001b[39mspawn\u001b[39m\u001b[39m'\u001b[39m\u001b[39m start method\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    210\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mhasattr\u001b[39m(torch\u001b[39m.\u001b[39m_C, \u001b[39m'\u001b[39m\u001b[39m_cuda_getDeviceCount\u001b[39m\u001b[39m'\u001b[39m):\n\u001b[0;32m--> 211\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mAssertionError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mTorch not compiled with CUDA enabled\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    212\u001b[0m \u001b[39mif\u001b[39;00m _cudart \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    213\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mAssertionError\u001b[39;00m(\n\u001b[1;32m    214\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mlibcudart functions unavailable. It looks like you have a broken build?\u001b[39m\u001b[39m\"\u001b[39m)\n","\u001b[0;31mAssertionError\u001b[0m: Torch not compiled with CUDA enabled"]}],"source":["# Initialize tokenizers, model, loss, optimizer\n","if USE_BERT:\n","  tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-uncased')\n","  model = BertLMHeadModel.from_pretrained(\"bert-base-multilingual-uncased\").to(device)\n","else:\n","  model = T5ForConditionalGeneration.from_pretrained(\"google/byt5-small\").to(device)\n","  tokenizer = AutoTokenizer.from_pretrained(\"google/byt5-small\",\n","                                            output_scores=True,\n","                                            output_hidden_states=True)\n","\n","nll_loss = nn.CrossEntropyLoss()\n","optimizer = AdamW(model.parameters(),\n","                  lr = 5e-5, # args.learning_rate - default is 5e-5\n","                  eps = 1e-8 # args.adam_epsilon  - default is 1e-8.\n","                )"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HtDk2OlaoXwo"},"outputs":[],"source":["model.train()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FJYR2z9TTTIN"},"outputs":[],"source":["for idx in range(1000):\n","  loss, steps = 0.0, 0\n","\n","  for i in range(df.shape[0]):\n","    model.zero_grad()\n","\n","    input = tokenizer(list(df.iloc[i]), return_tensors='pt', padding=True).to(device)\n","    output = model(input_ids    = input.input_ids[0].unsqueeze(0),\n","                  attention_mask = input.attention_mask[0].unsqueeze(0),\n","                  labels         = input.input_ids[1].unsqueeze(0))\n","\n","    output.loss.backward()\n","    optimizer.step()\n","    loss += float(output.loss)\n","    steps += 1\n","    # print(output.loss)\n","    # print(tokenizer.decode(torch.argmax(output.logits, axis=2)[0]))\n","    # print(df.iloc[i][1])\n","  print(idx, loss/steps)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":407},"executionInfo":{"elapsed":402,"status":"error","timestamp":1655434384729,"user":{"displayName":"Lj Flores","userId":"11095487892395270199"},"user_tz":240},"id":"n-jdDRWZlxM8","outputId":"c8e42dca-231e-4b66-f112-6dea9de2c46b"},"outputs":[{"ename":"ValueError","evalue":"ignored","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m<ipython-input-30-01a662308522>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m   output_1 = model(input_ids      = input_1.input_ids[0].unsqueeze(0),\n\u001b[0;32m---> 14\u001b[0;31m                    attention_mask = input_1.attention_mask[0].unsqueeze(0))\n\u001b[0m\u001b[1;32m     15\u001b[0m   output_2 = model.generate(input_ids      = input_2.input_ids[0].unsqueeze(0),\n\u001b[1;32m     16\u001b[0m                    attention_mask = input_2.attention_mask[0].unsqueeze(0))\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1108\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1111\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1112\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/models/t5/modeling_t5.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, decoder_input_ids, decoder_attention_mask, head_mask, decoder_head_mask, cross_attn_head_mask, encoder_outputs, past_key_values, inputs_embeds, decoder_inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1618\u001b[0m             \u001b[0moutput_attentions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_attentions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1619\u001b[0m             \u001b[0moutput_hidden_states\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_hidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1620\u001b[0;31m             \u001b[0mreturn_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreturn_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1621\u001b[0m         )\n\u001b[1;32m   1622\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1108\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1111\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1112\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/models/t5/modeling_t5.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, encoder_hidden_states, encoder_attention_mask, inputs_embeds, head_mask, cross_attn_head_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    898\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    899\u001b[0m             \u001b[0merr_msg_prefix\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"decoder_\"\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_decoder\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 900\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"You have to specify either {err_msg_prefix}input_ids or {err_msg_prefix}inputs_embeds\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    901\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    902\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0minputs_embeds\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mValueError\u001b[0m: You have to specify either decoder_input_ids or decoder_inputs_embeds"]}],"source":["for j in range(df_test.shape[0]):\n","\n","  i=10\n","  model.zero_grad()\n","  \n","  input_1 = tokenizer(df_test[0][i], \n","                      return_tensors='pt', \n","                      padding=True).to(device)\n","  input_2 = tokenizer(perturb_test_sent(df_test[0][i], vocab_tl), \n","                      return_tensors='pt', \n","                      padding=True).to(device)\n","\n","  output_1 = model(input_ids      = input_1.input_ids[0].unsqueeze(0),\n","                   attention_mask = input_1.attention_mask[0].unsqueeze(0))\n","  output_2 = model.generate(input_ids = input_2.input_ids[0].unsqueeze(0),\n","                   attention_mask = input_2.attention_mask[0].unsqueeze(0))\n","  \n","  # Backpropagate Squared Diff\n","  min_idx     = min(output_1.logits.shape[1],output_2.logits.shape[1])\n","  diff_tensor = output_1.logits[:,:min_idx,:]-output_2.logits[:,:min_idx,:]\n","  torch.mean(diff_tensor**2).backward()\n","\n","  print(\"Orig: \"+df_test[0][i])\n","  print(\"Output 1: \"+tokenizer.decode(torch.argmax(output_1.logits, axis=2)[0]))\n","  print(\"Output 1: \"+tokenizer.decode(torch.argmax(output_2.logits, axis=2)[0]))"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"executionInfo":{"elapsed":178,"status":"ok","timestamp":1655434386664,"user":{"displayName":"Lj Flores","userId":"11095487892395270199"},"user_tz":240},"id":"igJ_t_Xvj9gC","outputId":"90b075a6-4608-4786-f8bb-cb41b4841951"},"outputs":[{"data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'ung</s>'"]},"execution_count":31,"metadata":{},"output_type":"execute_result"}],"source":["tokenizer.decode(input_1.input_ids[0])"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"executionInfo":{"elapsed":266,"status":"ok","timestamp":1655434387069,"user":{"displayName":"Lj Flores","userId":"11095487892395270199"},"user_tz":240},"id":"7W1YQfQtYkZ7","outputId":"db73b5d2-f09a-41c0-80f5-718b0ec7e551"},"outputs":[{"data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'<pad>an</s>'"]},"execution_count":32,"metadata":{},"output_type":"execute_result"}],"source":["tokenizer.decode(model.generate(input_ids = input_1.input_ids[0].unsqueeze(0),\n","                                attention_mask = input_1.attention_mask[0].unsqueeze(0),\n","                                output_scores=True,\n","                                output_hidden_states=True)[0])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-NcaHsVUE73R"},"outputs":[],"source":["model.generate()"]}],"metadata":{"colab":{"authorship_tag":"ABX9TyNWQnRm+uqiuAInYaP0af/M","collapsed_sections":[],"mount_file_id":"1hWEbrphNNAgInXXHP2Lm4F1-gQlsT5nF","name":"main.ipynb","provenance":[]},"gpuClass":"standard","kernelspec":{"display_name":"Python 3.8.0 64-bit ('3.8.0')","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.0"},"vscode":{"interpreter":{"hash":"9791c6f58daa027bd1844a2b3b5f60294f44fc88849feb0463821081a914b57f"}}},"nbformat":4,"nbformat_minor":0}
